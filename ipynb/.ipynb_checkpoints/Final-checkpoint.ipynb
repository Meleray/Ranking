{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import locale\n",
    "from pymystem3 import Mystem\n",
    "from multiprocessing import Pool\n",
    "from p_tqdm import p_imap\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "import scipy.spatial.distance as ds\n",
    "import tqdm\n",
    "import torch\n",
    "from p_tqdm import p_map\n",
    "import fasttext.util\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(line):\n",
    "    l = line.split('\\t')\n",
    "    text = l[1].strip()\n",
    "    qid=l[0]\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(text.lower())\n",
    "    return qid,''.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "queries = {}\n",
    "pool = Pool(6)\n",
    "file = open(\"normal_queries.txt\", 'w', encoding = 'utf-8')\n",
    "with open(\"fspell.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    lines= f.readlines()\n",
    "    for qid, norm_text in p_map(stem,lines):\n",
    "        file.write(qid+'\\t'+norm_text)\n",
    "        queries[qid] = norm_text\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(6)\n",
    "file = open(\"normal_titles.txt\", 'w', encoding = 'utf-8')\n",
    "with open(\"title.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for i in tqdm.tqdm(range(3032)):\n",
    "        lines=[]\n",
    "        for _ in range(192):\n",
    "            lines.append(f.readline())\n",
    "        for qid, norm_text in pool.map(stem,lines):\n",
    "            file.write(qid+'\\t'+norm_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#редаектирование в ручную и на сайте яндекс-спеллер\n",
    "file =open(\"fspell.txt\" ,'r', encoding='utf-8')\n",
    "queries_not_normal = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вытащил в отдельный файл заголовки\n",
    "file =open(\"title.txt\" ,'r', encoding='utf-8')\n",
    "titles_not_normal= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles_not_normal[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_queries.txt\" ,'r', encoding='utf-8')\n",
    "queries = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    queries[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_titles.txt\" ,'r', encoding='utf-8')\n",
    "titles= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#гугл переводчик дает загружать и переводить файл\n",
    "file =open(\"translation_queries.txt\" ,'r', encoding='utf-8')\n",
    "tr_queries = {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    tr_queries[int(l[0])]= l[1].lower().strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"translation_titles.txt\" ,'r', encoding='utf-8')\n",
    "tr_titles= {}\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    tr_titles[int(l[0])]= l[1].strip()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202079/202079 [00:00<00:00, 342166.41it/s]\n",
      "100%|██████████| 403971/403971 [00:00<00:00, 419447.43it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"train.marks.tsv/train.marks.tsv\", 'r')\n",
    "train=[]\n",
    "train_map={}\n",
    "queries_doc_map_all={}\n",
    "marks={}\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split('\\t')\n",
    "    if int(splits[2])==0:\n",
    "        continue\n",
    "    train.append([int(splits[0]),int(splits[1]), int(splits[2])])\n",
    "    marks[splits[0]+\"|\"+splits[1]]=int(splits[2])\n",
    "    train_map[splits[0]+'|'+splits[1]]=splits[2].strip()\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "train=np.array(train)\n",
    "file.close()\n",
    "test_map={}\n",
    "file = open(\"sample.csv/sample.csv\", 'r')\n",
    "file.readline()\n",
    "test=[]\n",
    "for l in tqdm.tqdm(file.readlines()):\n",
    "    splits=l.split(',')\n",
    "    test.append([int(splits[0]),int(splits[1]), -1])\n",
    "    test_map[splits[0]+'|'+splits[1].strip()]=\"-1\"\n",
    "    if int(splits[0]) not in queries_doc_map_all.keys():\n",
    "        queries_doc_map_all[int(splits[0])]=[int(splits[1])]\n",
    "    else:\n",
    "        queries_doc_map_all[int(splits[0])].append(int(splits[1]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок с пользовательским поведением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_hosts={}\n",
    "set_hosts =set()\n",
    "with open(\"url.data/url.data\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        host=l[1].split(\"/\")[0]\n",
    "        host = host[4:] if host.startswith(\"www.\") else host\n",
    "        doc_id = int(l[0])\n",
    "        map_hosts[host]=doc_id\n",
    "        set_hosts.add(host)\n",
    "hosts={}\n",
    "with open(\"url.data/url.data\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        host=l[1].split(\"/\")[0]\n",
    "        host = host[4:] if host.startswith(\"www.\") else host\n",
    "        host_id = map_hosts[host]    \n",
    "        doc_id = int(l[0])\n",
    "        if host_id in hosts.keys():\n",
    "            hosts[host_id].append(doc_id)\n",
    "        else:\n",
    "            hosts[host_id] = []\n",
    "            hosts[host_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('features/IKS/') #нашел сайт, где для всех достал рейтинг ИКС от яндекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IKS={}\n",
    "for file in files:\n",
    "    with open('features/IKS/'+file, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f.readlines():\n",
    "            l=line.strip().split(\";\")\n",
    "            IKS[map_hosts[l[0]]]=[int(l[1]), np.log1p(int(l[1]))]\n",
    "            for host in hosts[map_hosts[l[0]]]:\n",
    "                IKS[host]=[int(l[1]), np.log1p(int(l[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(\"features/iks.txt\", IKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hosts.txt\", 'w', encoding='utf-8') as f:\n",
    "    for i in set_hosts:\n",
    "        s=f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_sDBN = {}\n",
    "file = open(\"click_data/new_sDBN/DOCS/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    d.append(float(splits[1]))\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    docs_sDBN[int(splits[0])]=np.array(d)\n",
    "file.close()\n",
    "qdocs_sDBN = {}\n",
    "file = open(\"click_data/new_sDBN/QDOCS/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    d.append(float(splits[4]))\n",
    "    qdocs_sDBN[splits[0]+\"|\"+splits[1]]=np.array(d)\n",
    "file.close()\n",
    "host_sDBN = {}\n",
    "file = open(\"click_data/new_sDBN/HOST/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    hid= int(splits[0])\n",
    "    d.append(float(splits[1]))\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    if hid in hosts.keys():\n",
    "        for host in hosts[hid]:\n",
    "            host_sDBN[host] =np.array(d)\n",
    "file.close()\n",
    "qhost_sDBN = {}\n",
    "file = open(\"click_data/new_sDBN/QHOST/part-r-00000\",'r')\n",
    "for l in file.readlines():\n",
    "    d=[]\n",
    "    splits=l.split('\\t')\n",
    "    hid= int(splits[1])\n",
    "    d.append(float(splits[2]))\n",
    "    d.append(float(splits[3]))\n",
    "    d.append(float(splits[4]))\n",
    "    if hid in hosts.keys():\n",
    "        for host in hosts[hid]:\n",
    "            if host in queries_doc_map_all[int(splits[0])]:\n",
    "                qhost_sDBN[splits[0]+\"|\"+str(host)]=np.array(d)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, Q=False):\n",
    "    Docs = {}\n",
    "    if Q:\n",
    "        start=2\n",
    "        end=13\n",
    "    else:\n",
    "        start=1\n",
    "        end=12\n",
    "    for i in range(11):\n",
    "        with open(\"click_data/new_PBM/\"+filename+\"/part-r-000\"+str(i).zfill(2), 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                data={}\n",
    "                l = line.strip().split('\\t')\n",
    "                for k in range(start,end):\n",
    "                    splt = l[k].strip().split(\":\")\n",
    "                    data[splt[0]]= float(splt[1])\n",
    "                for k in range(end,end+3):\n",
    "                    splt = l[k].split(\":\")\n",
    "                    pos = []\n",
    "                    for m,j in enumerate(splt[1].strip().split(\" \")):\n",
    "                        if m == 10:\n",
    "                            data[splt[0]+'_last']=int(j)\n",
    "                            break\n",
    "                        pos.append(int(j))\n",
    "                    data[splt[0]]=np.array(pos)\n",
    "                if Q:\n",
    "                    Docs[l[0]+'|'+l[1]]=data\n",
    "                else:\n",
    "                    Docs[int(l[0])]=data\n",
    "    return Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(Docs, Q=False,H=False):\n",
    "    features = {}\n",
    "    alpha = np.array([0.41,0.16,0.105,0.08,0.06,0.05,0.35,0.3,0.25,0.2])\n",
    "    for i in Docs.keys():\n",
    "        #if Docs[i]['clicks']<=1:\n",
    "        #    continue\n",
    "        CTR = Docs[i]['clicks']/Docs[i]['shows']\n",
    "        AvgTime = Docs[i]['time']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        FCTR = Docs[i]['first_click']/Docs[i]['shows']\n",
    "        LCTR = Docs[i]['last_click']/Docs[i]['shows']\n",
    "        AVGPos = np.mean(Docs[i]['shows_pos'])\n",
    "        AVGClick = np.mean(Docs[i]['click_pos'])\n",
    "        AVGPosClick = np.mean(Docs[i]['show_pos_if_click'])\n",
    "        Shows10 = Docs[i]['shows_pos_last']/Docs[i]['shows']\n",
    "        Clicks10 = Docs[i]['click_pos_last']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        LastProb = Docs[i]['last_click']/Docs[i]['clicks'] if Docs[i]['clicks']!=0 else 0\n",
    "        NotFirst = np.sum(Docs[i]['click_pos'][1:])/Docs[i]['shows']\n",
    "        CTRPos = 1.*Docs[i]['show_pos_if_click']/Docs[i]['shows_pos'] #массив\n",
    "        Prob_Click_pos = Docs[i]['click_pos']/Docs[i]['clicks'] #массив\n",
    "        Prob_Click_showpos= Docs[i]['show_pos_if_click']/Docs[i]['clicks'] #массив\n",
    "        PBM = Docs[i]['clicks']/np.sum(Docs[i]['shows_pos']*alpha)\n",
    "        CM_shows =  Docs[i]['cm_clicks']/Docs[i]['cm_shows']  if Docs[i]['cm_shows']!=0 else 0\n",
    "        UpProb = Docs[i]['up_click']/Docs[i]['shows']\n",
    "        DownProb = Docs[i]['down_click']/Docs[i]['shows']\n",
    "        AVGbeforeShowDocs=Docs[i]['before_click']/(Docs[i]['clicks']-Docs[i]['first_click']) if (Docs[i]['clicks']-Docs[i]['first_click'])!=0 else 0\n",
    "        AVGafterShowDocs=Docs[i]['after_click']/(Docs[i]['clicks']-Docs[i]['last_click'])if (Docs[i]['clicks']-Docs[i]['last_click'])!=0 else 0\n",
    "        CTR5=np.sum(Docs[i]['show_pos_if_click'][0:5])/np.sum(Docs[i]['shows_pos'][0:5]) if np.sum(Docs[i]['shows_pos'][0:5])!=0 else 0\n",
    "        logShows =  np.log(1+Docs[i]['shows'])\n",
    "        logClicks =  np.log(1+Docs[i]['clicks'])\n",
    "        data = [CTR, AvgTime, FCTR, LCTR, AVGPos, AVGClick, AVGPosClick, Shows10, Clicks10,LastProb,AVGbeforeShowDocs,AVGafterShowDocs,CTR5,\n",
    "               NotFirst,PBM, CM_shows, logClicks, logShows,UpProb,DownProb]\n",
    "        for k in CTRPos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "        for k in Prob_Click_pos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "        for k in Prob_Click_showpos:\n",
    "            if np.isnan(k):\n",
    "                k=0\n",
    "            data.append(k)\n",
    "            \n",
    "            \n",
    "        if Q and H:\n",
    "            s=i.split('|')\n",
    "            if int(s[1]) in hosts.keys():\n",
    "                for host in hosts[int(s[1])]:\n",
    "                    if host in queries_doc_map_all[int(s[0])]:\n",
    "                        features[s[0]+\"|\"+ str(host)] = data\n",
    "        \n",
    "        if H and ~Q:\n",
    "            if i in hosts.keys():\n",
    "                for host in hosts[i]:\n",
    "                    features[host] = data \n",
    "        if ~H:\n",
    "            features[i] = data\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_make(filename, Q, H):\n",
    "    Docs=read_data(filename, Q)\n",
    "    return make_features(Docs,Q=Q,H=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_docs =read_and_make(\"DOCS\", Q=False, H=False)\n",
    "features_hosts =read_and_make(\"HOST\", Q=False, H=True)\n",
    "features_qdocs=read_and_make(\"QDOCS\", Q=True, H=False)\n",
    "features_qhosts =read_and_make(\"QHOST\", Q=True, H=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = {}\n",
    "with open(\"click_data/QUERY/part-r-00000\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data={}\n",
    "        l = line.strip().split('\\t')\n",
    "        for k in range(1,len(l)):\n",
    "            splt = l[k].strip().split(\":\")\n",
    "            data[splt[0]]= float(splt[1])\n",
    "        Docs[int(l[0])]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_queries = {}\n",
    "for i in Docs.keys():\n",
    "    AVGDocs = Docs[i]['shows_docs']/Docs[i]['count_query']\n",
    "    AVGClicks = Docs[i]['clicks_docs']/Docs[i]['count_query']\n",
    "    AVGtime = Docs[i]['time']/Docs[i]['count_query'] if Docs[i]['time']<0 else 60*30\n",
    "    AVG_pos = Docs[i]['avg_pos_click']/Docs[i]['count_query']\n",
    "    AVG_pos_first = Docs[i]['first_click']/Docs[i]['count_query']\n",
    "    Noclick = Docs[i]['shows_noclick']\n",
    "    data=[AVGDocs,AVGClicks,AVGtime,AVG_pos,AVG_pos_first,Noclick]\n",
    "    features_queries[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(filename, features):\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        for i in features.keys():\n",
    "            f.write(str(i)+'\\t')\n",
    "            for j in features[i]:\n",
    "                if not np.isfinite(j) or j<0:\n",
    "                    j=0\n",
    "                f.write(str(j)+'\\t')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(\"features/click_docs.txt\", features_docs)\n",
    "save_features(\"features/click_qdocs.txt\", features_qdocs)\n",
    "save_features(\"features/click_hosts.txt\", features_hosts)\n",
    "save_features(\"features/click_qhosts.txt\", features_qhosts)\n",
    "save_features(\"features/click_q.txt\", features_queries)\n",
    "save_features(\"features/sDBN_docs.txt\", docs_sDBN)\n",
    "save_features(\"features/sDBN_qdocs.txt\", qdocs_sDBN)\n",
    "save_features(\"features/sDBN_hosts.txt\", host_sDBN)\n",
    "save_features(\"features/sDBN_qhosts.txt\", qhost_sDBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(filename):\n",
    "    fet={}\n",
    "    with open(filename,'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            l=line.strip().split('\\t')\n",
    "            x=[]\n",
    "            for i in range(1, len(l)):\n",
    "                x.append(float(l[i].strip()))\n",
    "            fet[l[0]]=x\n",
    "    return fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_docs=read_features(\"features/click_docs.txt\") \n",
    "features_qdocs=read_features(\"features/click_qdocs.txt\")\n",
    "features_hosts=read_features(\"features/click_hosts.txt\")\n",
    "features_qhosts=read_features(\"features/click_qhosts.txt\")\n",
    "features_queries=read_features(\"features/click_q.txt\")\n",
    "docs_sDBN=read_features(\"features/sDBN_docs.txt\")\n",
    "qdocs_sDBN=read_features(\"features/sDBN_qdocs.txt\")\n",
    "host_sDBN=read_features(\"features/sDBN_hosts.txt\")\n",
    "qhost_sDBN=read_features(\"features/sDBN_qhosts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок работы с заголовками и текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n",
    "from stop_words import get_stop_words\n",
    "stop = get_stop_words('ru')+get_stop_words('en')+['aren', 'can', \n",
    "    'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll',\n",
    "        'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n=1, word=True):\n",
    "    if word:\n",
    "        n_grams = ngrams(word_tokenize(text), n)\n",
    "        return [ ' '.join(grams) for grams in n_grams]\n",
    "    else:\n",
    "        n_grams = ngrams(text, n)\n",
    "        return [ ''.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_titles(doc_ids, bm=False, n=1, word=True, doc2vec=False):\n",
    "    for i in doc_ids:\n",
    "        try:\n",
    "            if bm:\n",
    "                yield get_ngrams(titles[i], n=n, word=word)\n",
    "            elif doc2vec:\n",
    "                yield i,titles[i]\n",
    "            else:\n",
    "                yield titles[i]\n",
    "        except:\n",
    "            if doc2vec:\n",
    "                yield i,\"0\"\n",
    "            else:\n",
    "                yield \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_titles_tr(doc_ids, bm=False, n=1, word=True, doc2vec=False):\n",
    "    for i in doc_ids:\n",
    "        try:\n",
    "            if bm:\n",
    "                yield get_ngrams(tr_titles[i], n=n, word=word)\n",
    "            elif doc2vec:\n",
    "                yield i,tr_titles[i]\n",
    "            else:\n",
    "                yield tr_titles[i]\n",
    "        except:\n",
    "            if doc2vec:\n",
    "                yield i,\"0\"\n",
    "            else:\n",
    "                yield \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тексты разделил и раскидал по файлам в пределах их групп. Преобразовал их при помощи pymorhy2\n",
    "def generator_doc(qid):\n",
    "    doc={}\n",
    "    f=open(\"docs/{}.txt\".format(qid), 'r', encoding='utf-8')\n",
    "    for line in f.readlines():\n",
    "        l=line.split('\\t')\n",
    "        doc[int(l[0])]=l[1].strip()\n",
    "    f.close()\n",
    "    for i in queries_doc_map_all[qid]:\n",
    "        if i in doc.keys():\n",
    "            yield doc[i]\n",
    "        else:\n",
    "            yield \"not found\"\n",
    "    del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_tfidf={}\n",
    "for i in tqdm.tqdm(range(0,len(queries_doc_map_all))):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=None)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos9=cosine(X,q).ravel()\n",
    "    cos1=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos2=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos10=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos3=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos11=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,13), stop_words=stop, analyzer='char_wb')\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos4=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos12=cosine(X,q).ravel()\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1),use_idf=False,stop_words=None)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos5=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos13=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop,use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos6=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos14=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop, use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos7=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos15=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(4,13), stop_words=stop, analyzer='char_wb', use_idf=False)\n",
    "    X=vectorizer.fit_transform(generator_titles(queries_doc_map_all[i]))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos8=(((X.dot(q.T)).toarray()).T).reshape(X.shape[0])\n",
    "    cos16=cosine(X,q).ravel()\n",
    "      \n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res1=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res2=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res3=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25Plus(sents)\n",
    "    res4=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25L(sents)\n",
    "    res5=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm2 = BM25L(sents)\n",
    "    res6=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25L(sents)\n",
    "    res7=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25L(sents)\n",
    "    res8=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=1))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res9=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=2))\n",
    "    bm2 = BM25Okapi(sents)\n",
    "    res10=bm25.get_scores(get_ngrams(queries[i], n=2, word=True))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=3, word=False))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res11=bm25.get_scores(get_ngrams(queries[i], n=3, word=False))\n",
    "    sents=list(generator_titles(queries_doc_map_all[i], bm=True, n=5, word=False))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res12=bm25.get_scores(get_ngrams(queries[i], n=5, word=False))\n",
    "    \n",
    "    for k,j in enumerate(queries_doc_map_all[i]):\n",
    "        features_tfidf[str(i)+\"|\"+str(j)]=[cos1[k],cos2[k], cos3[k],cos4[k],cos5[k],cos6[k], cos7[k],cos8[k],\n",
    "                       cos9[k],cos10[k], cos11[k],cos12[k],cos13[k],cos14[k], cos15[k],cos16[k],res1[k], res2[k], res3[k], res4[k],\n",
    "                                              res5[k], res6[k], res7[k], res8[k], res9[k], res10[k], res11[k], res12[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features('features/tfidf.txt', features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_tfidf_doc(i):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop)\n",
    "    X=vectorizer.fit_transform(generator_doc(i))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos1=cosine(X,q).ravel()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,5), stop_words=stop, analyzer='char_wb')\n",
    "    X=vectorizer.fit_transform(generator_doc(i))\n",
    "    q=vectorizer.transform([queries[i]])\n",
    "    cos2=cosine(X,q).ravel()\n",
    "    return i, np.vstack((cos1,cos2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf_doc={}\n",
    "for i,res in p_map(sim_tfidf_doc, range(6311)):\n",
    "    for k,j in zip(queries_doc_map_all[i], res):\n",
    "        features_tfidf_doc[str(i)+\"|\"+str(k)]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features('features/tfidf_ch_w_doc.txt', features_tfidf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_tfidf_doc(i):\n",
    "    sents=list(generator_doc(i))\n",
    "    bm25 = BM25Okapi(sents)\n",
    "    res=bm25.get_scores(get_ngrams(queries[i], n=1, word=True))\n",
    "    return i, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bm25doc={}\n",
    "for i,res in p_map(sim_tfidf_doc, range(6311)):\n",
    "    for k,j in zip(queries_doc_map_all[i], res):\n",
    "        features_bm25doc[str(i)+\"|\"+str(k)]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features('features/bm25doc.txt', features_bm25doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### док2век не стал добавлять как фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_2_vec(qid):\n",
    "    max_epochs = 10\n",
    "    alpha = 0.025\n",
    "    dist=[]\n",
    "    model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[j]) for j,doc in generator_titles(queries_doc_map_all[qid], doc2vec=True)]\n",
    "    tagged_data.append(TaggedDocument(words=word_tokenize(queries[qid]), tags=[\"q\"]))\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    for j in queries_doc_map_all[qid]:\n",
    "        dist.append([cosine(model.docvecs[j].reshape(1,-1),model.docvecs['q'].reshape(1,-1)).ravel()[0],np.dot(model.docvecs[j],model.docvecs['q'])])\n",
    "    return qid, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2_vec(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_doc2vec={}\n",
    "for i,k in p_map(doc_2_vec, range(6311)):\n",
    "    for m,n in zip(query_doc_map_all[qid],k):\n",
    "        features_doc2vec[str(i)+\"|\"+str(m)]=n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "конец док2век"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import gensim\n",
    "import zipfile\n",
    "model1=gensim.models.KeyedVectors.load(\"181/model.model\")\n",
    "model2=gensim.models.KeyedVectors.load(\"187/model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "ft_en = fasttext.load_model('cc.en.300.bin')\n",
    "ft_ru = fasttext.load_model('cc.ru.300.bin')\n",
    "ft_new= fasttext.load_model('ft_native_300_ru_wiki_lenta_lemmatize.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(qid):\n",
    "    res=[]\n",
    "    q_vector1=np.zeros(300)\n",
    "    q_vector2=np.zeros(300)\n",
    "    q_vector3=np.zeros(300)\n",
    "    q_vector4=np.zeros(300)\n",
    "    q_vector8=np.zeros(300)\n",
    "    q_vector5=ft_ru.get_sentence_vector(queries[qid])\n",
    "    q_vector6=ft_en.get_sentence_vector(queries[qid])\n",
    "    q_vector7=ft_new.get_sentence_vector(queries[qid])\n",
    "    for w in queries[qid].split():\n",
    "        q_vector1+=model1.get_vector(w)\n",
    "        q_vector2+=model2.get_vector(w)\n",
    "        q_vector3+=ft_ru.get_word_vector(w)\n",
    "        q_vector4+=ft_en.get_word_vector(w)\n",
    "        q_vector8+=ft_new.get_word_vector(w)\n",
    "    for title in list(generator_titles(queries_doc_map_all[qid])):\n",
    "        d_vector1=np.zeros(300)\n",
    "        d_vector2=np.zeros(300)\n",
    "        d_vector3=np.zeros(300)\n",
    "        d_vector4=np.zeros(300)\n",
    "        d_vector8=np.zeros(300)\n",
    "        d_vector5=ft_ru.get_sentence_vector(title)\n",
    "        d_vector6=ft_en.get_sentence_vector(title)\n",
    "        d_vector7=ft_new.get_sentence_vector(title)\n",
    "        for w in title.split():\n",
    "            d_vector1+=model1.get_vector(w)\n",
    "            d_vector2+=model2.get_vector(w)\n",
    "            d_vector3+=ft_ru.get_word_vector(w)\n",
    "            d_vector4+=ft_en.get_word_vector(w)\n",
    "            d_vector8+=ft_new.get_word_vector(w)\n",
    "        s=[model1.similarity(queries[qid], title),\n",
    "        model2.similarity(queries[qid], title),\n",
    "        ds.cosine(q_vector1,d_vector1),\n",
    "        ds.cosine(q_vector2,d_vector2),\n",
    "        ds.cosine(q_vector3,d_vector3),\n",
    "        ds.cosine(q_vector4,d_vector4),\n",
    "        ds.cosine(q_vector5,d_vector5),\n",
    "        ds.cosine(q_vector6,d_vector6),\n",
    "        ds.cosine(q_vector7,d_vector7),\n",
    "        ds.cosine(q_vector8,d_vector8),\n",
    "        np.dot(q_vector1,d_vector1),\n",
    "        np.dot(q_vector2,d_vector2),\n",
    "        np.dot(q_vector3,d_vector3),\n",
    "        np.dot(q_vector4,d_vector4),\n",
    "        np.dot(q_vector5,d_vector5),\n",
    "        np.dot(q_vector6,d_vector6),\n",
    "        np.dot(q_vector7,d_vector7),\n",
    "        np.dot(q_vector8,d_vector8)]\n",
    "        res.append(s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_fasttext={}\n",
    "for qid in tqdm.tqdm(range(6311)):\n",
    "    for i,j in zip(queries_doc_map_all[qid], sim(qid)):\n",
    "        features_fasttext[str(qid)+\"|\"+str(i)]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(\"features/fasttext.txt\", features_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"normal_titles.txt\" ,'r', encoding='utf-8')\n",
    "titles= []\n",
    "id_titles=[]\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titles.append(l[1].strip())\n",
    "    id_titles.append(int(l[0]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "model1 = SentenceTransformer('distiluse-base-multilingual-cased', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_from_model1=model1.encode(titles, show_progress_bar=True, batch_size=128)\n",
    "titles_={}\n",
    "for i,j in zip(id_titles,vectors_from_model1):\n",
    "    titles_[i]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features/bert.txt\", 'w', encoding='utf-8') as f:\n",
    "    for qid in range(len(queries)):\n",
    "        q_emb=model1.encode([queries[qid]])[0]\n",
    "        for i in queries_doc_map_all[qid]:\n",
    "            if i in titles_.keys():\n",
    "                f.write(str(qid)+\"|\"+str(i)+'\\t'+str(ds.cosine(q_emb, titles_[i]))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open(\"translation_titles.txt\" ,'r', encoding='utf-8')\n",
    "titless= []\n",
    "id_titles=[]\n",
    "for line in file.readlines():\n",
    "    l=line.split('\\t')\n",
    "    titless.append(l[1].strip())\n",
    "    id_titles.append(int(l[0]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('bert-large-nli-mean-tokens', device='cuda')\n",
    "vectors_from_model1=model1.encode(titless, show_progress_bar=True, batch_size=128)\n",
    "titles_={}\n",
    "for i,j in zip(id_titles,vectors_from_model1):\n",
    "    titles_[i]=j\n",
    "with open(\"features/english_bert.txt\", 'w', encoding='utf-8') as f:\n",
    "    for qid in range(len(queries)):\n",
    "        q_emb=model1.encode([tr_queries[qid]])[0]\n",
    "        for i in queries_doc_map_all[qid]:\n",
    "            if i in titles_.keys():\n",
    "                f.write(str(qid)+\"|\"+str(i)+'\\t'+str(ds.cosine(q_emb, titles_[i]))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('roberta-large-nli-stsb-mean-tokens', device='cuda')\n",
    "vectors_from_model1=model1.encode(titless, show_progress_bar=True, batch_size=128)\n",
    "titles_={}\n",
    "for i,j in zip(id_titles,vectors_from_model1):\n",
    "    titles_[i]=j\n",
    "with open(\"features/english_bert_2.txt\", 'w', encoding='utf-8') as f:\n",
    "    for qid in tqdm.tqdm(range(len(queries))):\n",
    "        q_emb=model1.encode([tr_queries[qid]])[0]\n",
    "        for i in queries_doc_map_all[qid]:\n",
    "            if i in titles_.keys():\n",
    "                f.write(str(qid)+\"|\"+str(i)+'\\t'+str(ds.cosine(q_emb, titles_[i]))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert1=read_features(\"features/bert.txt\")\n",
    "features_bert2=read_features(\"features/english_bert_2.txt\")\n",
    "features_bert3=read_features(\"features/english_bert.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert={}\n",
    "for i in features_bert1.keys():\n",
    "    x2 = features_bert2[i][0] if i in features_bert2.keys() else 1\n",
    "    x3 = features_bert3[i][0] if i in features_bert3.keys() else 1\n",
    "    features_bert[i]=[features_bert1[i][0],x2,x3]\n",
    "save_features('features/bert_all.txt', features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "embed1 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "embed2 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "embed3 = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "module3 = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "module4= hub.load('https://tfhub.dev/google/universal-sentence-encoder-qa/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_USE(qid):\n",
    "    questions = [queries[qid]]\n",
    "    question_embeddings = module3.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    responses = list(generator_titles(queries_doc_map_all[qid]))\n",
    "    response_contexts = list(generator_doc(qid))\n",
    "    response_embeddings = module3.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim1 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    questions = [tr_queries[qid]]\n",
    "    responses = list(generator_titles_tr(queries_doc_map_all[qid]))\n",
    "    response_contexts = list(generator_titles_tr(queries_doc_map_all[qid]))\n",
    "    question_embeddings = module4.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    response_embeddings = module4.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim4 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    sim2 = cosine(embed1(queries[qid]), embed1(list(generator_titles(queries_doc_map_all[qid])))).ravel()\n",
    "    sim3 = cosine(embed2(queries[qid]), embed2(list(generator_titles(queries_doc_map_all[qid])))).ravel()\n",
    "    sim5 = cosine(embed3([tr_queries[qid]])[0].numpy().reshape(1,-1), embed3(list(generator_titles_tr(queries_doc_map_all[qid]))).numpy()).ravel()\n",
    "    return np.hstack((sim1.reshape(-1,1),sim2.reshape(-1,1),sim3.reshape(-1,1), sim4.reshape(-1,1), sim5.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_USE={}\n",
    "for qid in tqdm.tqdm(range(6311)):\n",
    "    for i,j in zip(queries_doc_map_all[qid], sim_USE(qid)):\n",
    "        features_USE[str(qid)+\"|\"+str(i)]=j\n",
    "save_features(\"features/USE_5.txt\", features_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_USE(qid):\n",
    "    questions = [queries_not_normal[qid]]\n",
    "    question_embeddings = module3.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "    responses = list(generator_titles_not_normal(queries_doc_map_all[qid]))\n",
    "    response_contexts = list(generator_titles_not_normal(queries_doc_map_all[qid]))\n",
    "    response_embeddings = module3.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "    sim1 = np.inner(question_embeddings['outputs'], response_embeddings['outputs']).ravel()\n",
    "    sim2 = cosine(embed1(queries_not_normal[qid]), embed1(list(generator_titles_not_normal(queries_doc_map_all[qid])))).ravel()\n",
    "    sim3 = cosine(embed2(queries_not_normal[qid]), embed2(list(generator_titles_not_normal(queries_doc_map_all[qid])))).ravel()\n",
    "    return np.hstack((sim1.reshape(-1,1),sim2.reshape(-1,1),sim3.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_USE_not_normal={}\n",
    "with open(\"features/USE_not_normal.txt\", 'a') as f:\n",
    "    for qid in tqdm.tqdm(range(6311)):\n",
    "        for i,j in zip(queries_doc_map_all[qid], sim_USE(qid)):\n",
    "            features_USE_not_normal[str(qid)+\"|\"+str(i)]=j\n",
    "            f.write(str(qid)+\"|\"+str(i)+'\\t'+str(j[0])+'\\t'+str(j[1])+'\\t'+str(j[2])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### блок фасттекст на текстах и переведенных заголовках, запросах. не стал добавлять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_doc(qid):\n",
    "    q_vector5=ft_ru.get_sentence_vector(queries[qid])\n",
    "    q_vector6=ft_en.get_sentence_vector(tr_queries[qid])\n",
    "    q_vector7=ft_new.get_sentence_vector(queries[qid])\n",
    "    trs=list(generator_titles_tr(queries_doc_map_all[qid]))\n",
    "    docs=list(generator_doc(qid))\n",
    "    res=[]\n",
    "    for i in range(len(queries_doc_map_all[qid])):\n",
    "        d_vector5=ft_ru.get_sentence_vector(docs[i])\n",
    "        d_vector6=ft_en.get_sentence_vector(trs[i])\n",
    "        d_vector7=ft_new.get_sentence_vector(docs[i])\n",
    "        s=[ds.cosine(q_vector5,d_vector5),\n",
    "        ds.cosine(q_vector6,d_vector6),\n",
    "        ds.cosine(q_vector7,d_vector7),\n",
    "        np.dot(q_vector5,d_vector5),\n",
    "        np.dot(q_vector6,d_vector6),\n",
    "        np.dot(q_vector7,d_vector7)]\n",
    "        res.append(s)\n",
    "    return qid,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fasttext_doc_and_translate={}\n",
    "with open(\"features/fasttext_doc_and_translate.txt\", 'a', encoding='utf-8') as f:\n",
    "    for qid in tqdm.tqdm(range(2497,6311)):\n",
    "        for i,j in zip(queries_doc_map_all[qid], sim(qid)):\n",
    "            f.write(str(qid)+\"|\"+str(i)+'\\t'+str(j[0])+'\\t'+str(j[1])+'\\t'+str(j[2])+'\\t'+str(j[3])+'\\t'+str(j[4])+'\\t'+str(j[5])+'\\n')\n",
    "            features_fasttext_doc_and_translate[str(qid)+\"|\"+str(i)]=j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### конец блока фасттекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\", use_fast=True)\n",
    "model1 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\", use_fast=True)\n",
    "model2 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\", use_fast=True)\n",
    "model3 = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_bert(qid):\n",
    "    res=[]\n",
    "    q_token = tokenizer1.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb1 = model1(**q_token)[0].to(torch.device('cpu'))\n",
    "    q_token = tokenizer2.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb2 = model2(**q_token)[0].to(torch.device('cpu'))\n",
    "    q_token = tokenizer3.encode_plus(queries[qid], return_tensors=\"pt\").to(device)\n",
    "    q_emb3 = model3(**q_token)[0].to(torch.device('cpu'))\n",
    "    for title in generator_titles(queries_doc_map_all[qid]):\n",
    "        d_token = tokenizer1.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb1 = model1(**d_token)[0].to(torch.device('cpu'))\n",
    "        d_token = tokenizer2.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb2 = model2(**d_token)[0].to(torch.device('cpu'))\n",
    "        d_token = tokenizer3.encode_plus(title, return_tensors=\"pt\").to(device)\n",
    "        d_emb3 = model3(**d_token)[0].to(torch.device('cpu'))\n",
    "        res.append([cosine(q_emb1[0], d_emb1[0]).mean(),cosine(q_emb1.sum(1), d_emb1.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb2[0], d_emb2[0]).mean(),cosine(q_emb2.sum(1), d_emb2.sum(1)).ravel()[0],\n",
    "                   cosine(q_emb3[0], d_emb3[0]).mean(),cosine(q_emb3.sum(1), d_emb3.sum(1)).ravel()[0]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open(\"features/transforms.txt\", 'a', encoding='utf-8')\n",
    "for qid in tqdm.tqdm(range(0,6311)):\n",
    "    for i,j in zip(queries_doc_map_all[qid], sim_bert(qid)):\n",
    "        file.write(str(qid)+\"|\"+str(i)+'\\t')\n",
    "        for k in j:\n",
    "            file.write(str(k)+'\\t')\n",
    "        file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf=read_features(\"features/tfidf.txt\")\n",
    "features_tfidf_cw_doc = read_features(\"features/tfidf_ch_w_doc.txt\")\n",
    "features_fasttext=read_features(\"features/fasttext.txt\")\n",
    "features_bert=read_features(\"features/bert_all.txt\")\n",
    "features_USE=read_features(\"features/USE_5.txt\")\n",
    "features_transforms = read_features(\"features/transforms.txt\")\n",
    "features_bm25doc=read_features(\"features/bm25doc.txt\")\n",
    "features_fasttext_doc_and_translate= read_features(\"features/fasttext_doc_and_translate.txt\")\n",
    "features_bert_not_normal=read_features(\"features/bert_not_normal.txt\")\n",
    "features_USE_not_normal=read_features(\"features/USE_not_normal.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дальше идет блок только с моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(pred_qids, preds, all_doc_ids, filename, train=False):\n",
    "    fout = open(filename, 'w')\n",
    "    fout.write('QueryId,DocumentId\\n')\n",
    "    for qid in np.unique(pred_qids):\n",
    "        q_doc_idxs = np.argwhere(pred_qids == qid).ravel()\n",
    "        doc_ids = copy.deepcopy(all_doc_ids[q_doc_idxs])\n",
    "        q_doc_scores = preds[q_doc_idxs]\n",
    "        sorted_doc_ids = doc_ids[np.argsort(q_doc_scores)[::-1]]\n",
    "        for i,did in enumerate(sorted_doc_ids):\n",
    "            if i==5:\n",
    "                break\n",
    "            if train:\n",
    "                fout.write('{0},{1},{2}\\n'.format(qid, did, train_map[str(qid)+\"|\"+str(did)]))\n",
    "            else:\n",
    "                fout.write('{0},{1}\\n'.format(qid, did))\n",
    "        \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features(t):\n",
    "    features = {}\n",
    "    for i in t:\n",
    "        x1=features_tfidf[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf.keys() else np.zeros(len(features_tfidf['0|340485']))\n",
    "        x2=features_fasttext[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_fasttext.keys() else np.zeros(len(features_fasttext['0|6760']))\n",
    "        x3=features_tfidf_cw_doc[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf_cw_doc.keys() else np.zeros(len(features_tfidf_cw_doc['0|6830']))\n",
    "        x4=features_hosts[str(i[1])] if str(i[1]) in features_hosts.keys() else np.zeros(len(features_hosts['10059']))\n",
    "        x5=features_docs[str(i[1])] if str(i[1]) in features_docs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x6=qdocs_sDBN[str(i[0])+\"|\"+str(i[1])]  if str(i[0])+\"|\"+str(i[1]) in qdocs_sDBN.keys() else np.zeros(3)\n",
    "        x7=qhost_sDBN[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in qhost_sDBN.keys() else np.zeros(3)\n",
    "        x8=docs_sDBN[str(i[1])] if str(i[1]) in docs_sDBN.keys() else np.zeros(3)\n",
    "        x9=host_sDBN[str(i[1])] if str(i[1]) in host_sDBN.keys() else np.zeros(3)\n",
    "        x10=features_qhosts[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qhosts.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x11=features_qdocs[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_qdocs.keys() else np.zeros(len((features_docs['100059'])))\n",
    "        x12=features_queries[str(i[0])] if str(i[0]) in features_queries.keys() else np.zeros(6)\n",
    "        x13=features_USE[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_USE.keys() else np.zeros(len(features_USE['0|340485']))\n",
    "        x14=features_bert[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bert.keys() else np.zeros(len(features_bert['0|340485']))\n",
    "        x15=features_transforms[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_transforms.keys() else np.zeros(len(features_transforms['0|340485']))\n",
    "        x16=features_bm25doc[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bm25doc.keys() else np.zeros(len(features_bm25doc['0|340485']))\n",
    "        #x17=features_fasttext_doc_and_translate[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_fasttext_doc_and_translate.keys() else np.zeros(len(features_fasttext_doc_and_translate['0|340485']))\n",
    "        x18=features_bert_not_normal[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_bert_not_normal.keys() else np.zeros(len(features_bert_not_normal['0|340485']))\n",
    "        x19 = features_USE_not_normal[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_USE_not_normal.keys() else np.zeros(len(features_USE_not_normal['0|340485']))\n",
    "        #x20 = features_tfidf_translation[str(i[0])+\"|\"+str(i[1])] if str(i[0])+\"|\"+str(i[1]) in features_tfidf_translation.keys() else np.zeros(len(features_tfidf_translation['0|340485']))\n",
    "        features[str(i[0])+\"|\"+str(i[1])+\"|\"+str(i[2])]=np.hstack((x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x18,x19))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_train=all_features(train)\n",
    "features_test=all_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features={\n",
    "    0:\"скалярное * TFIDF по слову\",\n",
    "    1:\"скалярное * TFIDF по 2 словам\",\n",
    "    2:\"скалярное * TFIDF 1-3 нграммы\",\n",
    "    3:\"скалярное * TFIDF по 4-13 буквенным нграммам\",\n",
    "    4:\"скалярное * TF по слову\",\n",
    "    5:\"скалярное * TF по 2 словам\",\n",
    "    6:\"скалярное * TF 1-3 нграммы\",\n",
    "    7:\"скалярное * TF по 4-13 буквенным нграммам\",\n",
    "    8:\"косинус * TFIDF по слову\",\n",
    "    9:\"косинус * TFIDF по 2 словам\",\n",
    "    10:\"косинус * TFIDF 1-3 нграммы\",\n",
    "    11:\"косинус * TFIDF по 4-13 буквенным нграммам\",\n",
    "    12:\"косинус * TF по слову\",\n",
    "    13:\"косинус * TF по 2 словам\",\n",
    "    14:\"косинус * TF 1-3 нграммы\",\n",
    "    15:\"косинус * TF по 4-13 буквенным нграммам\",\n",
    "    16: \"BM25plus по слову\",\n",
    "    17: \"BM25Plus по 2 словам\",\n",
    "    18: \"BM25Plus по 3граммам буквенным\",\n",
    "    19: \"BM25Plus по 5граммам буквенным\",\n",
    "    20: \"BM25L по слову\",\n",
    "    21: \"BM25L по 2 словам\",\n",
    "    22: \"BM25L по 3граммам буквенным\",\n",
    "    23: \"BM25L по 5граммам буквенным\",\n",
    "    24: \"BM25kapi по слову\",\n",
    "    25: \"BM25kapi по 2 словам\",\n",
    "    26: \"BM25kapi по 3граммам буквенным\",\n",
    "    27: \"BM25kapi по 5граммам буквенным\",\n",
    "    28: \"fasttext RusVectores 181/model.model встроенная симилярити\",\n",
    "    29: \"fasttext RusVectores 187/model.model встроенная симилярити\",\n",
    "    30: \"fasttext RusVectores 181/model.model сумма векторов слов косинус\",\n",
    "    31: \"fasttext RusVectores 187/model.model сумма векторов слов косинус\",\n",
    "    32: \"fasttext cc.ru.300.bin сумма векторов слов косинус\",\n",
    "    33: \"fasttext cc.en.300.bin сумма векторов слов косинус\",\n",
    "    34: \"fasttext cc.ru.300.bin фразовый вектор косинус\",\n",
    "    35: \"fasttext cc.en.300.bin фразовый вектор косинус\",\n",
    "    36: \"fasttext ft_native_300_ru_wiki_lenta_lemmatize.bin фраза косинус\",\n",
    "    37: \"fasttext ft_native_300_ru_wiki_lenta_lemmatize.bin сумма векторов слов косинус\",\n",
    "    38: \"fasttext RusVectores 181/model.model сумма векторов слов скадярное\",\n",
    "    39: \"fasttext RusVectores 187/model.model сумма векторов слов скалярное\",\n",
    "    40: \"fasttext cc.ru.300.bin сумма векторов слов скалярное\",\n",
    "    41: \"fasttext cc.en.300.bin сумма векторов слов  скалярное\",\n",
    "    42: \"fasttext cc.ru.300.bin фразовый вектор скалярное\",\n",
    "    43: \"fasttext cc.en.300.bin фразовый вектор скалярное\",\n",
    "    44: \"fasttext ft_native_300_ru_wiki_lenta_lemmatize.bin фраза скалярное\",\n",
    "    45: \"fasttext ft_native_300_ru_wiki_lenta_lemmatize.bin сумма векторов слов скалярное\",\n",
    "    46: \"косинус TFIDF по текстам нграммы 1-5\",\n",
    "    47: \"косинус TFIDF по текстам словам 1-3\",\n",
    "    48: \"Хост CTR\",\n",
    "    49: \"Хост AVGtime\",\n",
    "    50: \"Хост FirstCTR\",\n",
    "    51: \"Хост LastCTR\",\n",
    "    52: \"Хост AVG Position\",\n",
    "    53: \"Хост AVG Position Click\",\n",
    "    54: \"Хост AVG showPosition if was Click\",\n",
    "    55: \"Хост Количество показов за пределами топ 10/на все показы\",\n",
    "    56: \"Хост Количество кликов за пределами топ 10/на все клики\",\n",
    "    57: \"Хост LastProb\",\n",
    "    58: \"Хост AVG click before show doc\",\n",
    "    59: \"Хост AVG click after show doc\",\n",
    "    60: \"Хост CTR@5\",\n",
    "    61: \"Хост Количество не первых кликов/на все клики\",\n",
    "    62: \"Хост PBM\",\n",
    "    63: \"Хост CM\",\n",
    "    64: \"Хост log(1+shows)\",\n",
    "    65: \"Хост log(1+clicks)\",\n",
    "    66: \"Хост UpProb\",\n",
    "    67: \"Хост DowbProb\",\n",
    "    68: \"Хост CTR position 1\",\n",
    "    69: \"Хост CTR position 2\",\n",
    "    70: \"Хост CTR position 3\",\n",
    "    71: \"Хост CTR position 4\",\n",
    "    72: \"Хост CTR position 5\",\n",
    "    73: \"Хост CTR position 6\",\n",
    "    74: \"Хост CTR position 7\",\n",
    "    75: \"Хост CTR position 8\",\n",
    "    76: \"Хост CTR position 9\",\n",
    "    77: \"Хост CTR position 10\",\n",
    "    78: \"Хост ProbClick clickposition 1\",\n",
    "    79: \"Хост ProbClick clickposition 2\",\n",
    "    80: \"Хост ProbClick clickposition 3\",\n",
    "    81: \"Хост ProbClick clickposition 4\",\n",
    "    82: \"Хост ProbClick clickposition 5\",\n",
    "    83: \"Хост ProbClick clickposition 6\",\n",
    "    84: \"Хост ProbClick clickposition 7\",\n",
    "    85: \"Хост ProbClick clickposition 8\",\n",
    "    86: \"Хост ProbClick clickposition 9\",\n",
    "    87: \"Хост ProbClick clickposition 10\",\n",
    "    88: \"Хост Probclick showposition 1\",\n",
    "    89: \"Хост Probclick showposition 2\",\n",
    "    90: \"Хост Probclick showposition 3\",\n",
    "    91: \"Хост Probclick showposition 4\",\n",
    "    92: \"Хост Probclick showposition 5\",\n",
    "    93: \"Хост Probclick showposition 6\",\n",
    "    94: \"Хост Probclick showposition 7\",\n",
    "    95: \"Хост Probclick showposition 8\",\n",
    "    96: \"Хост Probclick showposition 9\",\n",
    "    97: \"Хост Probclick showposition 10\",\n",
    "    98: \"Документ CTR\",\n",
    "    99: \"Документ AVGtime\",\n",
    "    100: \"Документ FirstCTR\",\n",
    "    101: \"Документ LastCTR\",\n",
    "    102: \"Документ AVG Position\",\n",
    "    103: \"Документ AVG Position Click\",\n",
    "    104: \"Документ AVG showPosition if was Click\",\n",
    "    105: \"Документ Количество показов за пределами топ 10/на все показы\",\n",
    "    106: \"Документ Количество кликов за пределами топ 10/на все клики\",\n",
    "    107: \"Документ LastProb\",\n",
    "    108: \"Документ AVG click before show doc\",\n",
    "    109: \"Документ AVG click after show doc\",\n",
    "    110: \"Документ CTR@5\",\n",
    "    111: \"Документ Количество не первых кликов/на все клики\",\n",
    "    112: \"Документ PBM\",\n",
    "    113: \"Документ CM\",\n",
    "    114: \"Документ log(1+shows)\",\n",
    "    115: \"Документ log(1+clicks)\",\n",
    "    116: \"Документ UpProb\",\n",
    "    117: \"Документ DowbProb\",\n",
    "    118: \"Документ CTR position 1\",\n",
    "    119: \"Документ CTR position 2\",\n",
    "    120: \"Документ CTR position 3\",\n",
    "    121: \"Документ CTR position 4\",\n",
    "    122: \"Документ CTR position 5\",\n",
    "    123: \"Документ CTR position 6\",\n",
    "    124: \"Документ CTR position 7\",\n",
    "    125: \"Документ CTR position 8\",\n",
    "    126: \"Документ CTR position 9\",\n",
    "    127: \"Документ CTR position 10\",\n",
    "    128: \"Документ ProbClick clickposition 1\",\n",
    "    129: \"Документ ProbClick clickposition 2\",\n",
    "    130: \"Документ ProbClick clickposition 3\",\n",
    "    131: \"Документ ProbClick clickposition 4\",\n",
    "    132: \"Документ ProbClick clickposition 5\",\n",
    "    133: \"Документ ProbClick clickposition 6\",\n",
    "    134: \"Документ ProbClick clickposition 7\",\n",
    "    135: \"Документ ProbClick clickposition 8\",\n",
    "    136: \"Документ ProbClick clickposition 9\",\n",
    "    137: \"Документ ProbClick clickposition 10\",\n",
    "    138: \"Документ Probclick showposition 1\",\n",
    "    139: \"Документ Probclick showposition 2\",\n",
    "    140: \"Документ Probclick showposition 3\",\n",
    "    141: \"Документ Probclick showposition 4\",\n",
    "    142: \"Документ Probclick showposition 5\",\n",
    "    143: \"Документ Probclick showposition 6\",\n",
    "    144: \"Документ Probclick showposition 7\",\n",
    "    145: \"Документ Probclick showposition 8\",\n",
    "    146: \"Документ Probclick showposition 9\",\n",
    "    147: \"Документ Probclick showposition 10\",\n",
    "    148: \"Запрос-докумет sDBN A\",\n",
    "    149: \"Запрос-документ sDBN S\",\n",
    "    150: \"Запрос-документ sDBN R\",\n",
    "    151: \"Запрос-хост sDBN A\",\n",
    "    152: \"Запрос-хост sDBN S\",\n",
    "    153: \"Запрос-хост sDBN R\",\n",
    "    154: \"Документ sDBN A\",\n",
    "    155: \"документ sDBN S\",\n",
    "    156: \"документ sDBN R\",\n",
    "    157: \"хост sDBN A\",\n",
    "    158: \"хост sDBN S\",\n",
    "    159: \"хост sDBN R\",\n",
    "    160: \"Запрос-Хост CTR\",\n",
    "    161: \"Запрос-Хост AVGtime\",\n",
    "    162: \"Запрос-Хост FirstCTR\",\n",
    "    163: \"Запрос-Хост LastCTR\",\n",
    "    164: \"Запрос-Хост AVG Position\",\n",
    "    165: \"Запрос-Хост AVG Position Click\",\n",
    "    166: \"Запрос-Хост AVG showPosition if was Click\",\n",
    "    167: \"Запрос-Хост Количество показов за пределами топ 10/на все показы\",\n",
    "    168: \"Запрос-Хост Количество кликов за пределами топ 10/на все клики\",\n",
    "    169: \"Запрос-Хост LastProb\",\n",
    "    170: \"Запрос-Хост AVG click before show doc\",\n",
    "    171: \"Запрос-Хост AVG click after show doc\",\n",
    "    172: \"Запрос-Хост CTR@5\",\n",
    "    173: \"Запрос-Хост Количество не первых кликов/на все клики\",\n",
    "    174: \"Запрос-Хост PBM\",\n",
    "    175: \"Запрос-Хост CM\",\n",
    "    176: \"Запрос-Хост log(1+shows)\",\n",
    "    177: \"Запрос-Хост log(1+clicks)\",\n",
    "    178: \"Запрос-Хост UpProb\",\n",
    "    179:\"Запрос-Хост DowbProb\",\n",
    "    180: \"Запрос-Хост CTR position 1\",\n",
    "    181: \"Запрос-Хост CTR position 2\",\n",
    "    182: \"Запрос-Хост CTR position 3\",\n",
    "    183: \"Запрос-Хост CTR position 4\",\n",
    "    184: \"Запрос-Хост CTR position 5\",\n",
    "    185: \"Запрос-Хост CTR position 6\",\n",
    "    186: \"Запрос-Хост CTR position 7\",\n",
    "    187: \"Запрос-Хост CTR position 8\",\n",
    "    188: \"Запрос-Хост CTR position 9\",\n",
    "    189: \"Запрос-Хост CTR position 10\",\n",
    "    190: \"Запрос-Хост ProbClick clickposition 1\",\n",
    "    191: \"Запрос-Хост ProbClick clickposition 2\",\n",
    "    192: \"Запрос-Хост ProbClick clickposition 3\",\n",
    "    193: \"Запрос-Хост ProbClick clickposition 4\",\n",
    "    194: \"Запрос-Хост ProbClick clickposition 5\",\n",
    "    195: \"Запрос-Хост ProbClick clickposition 6\",\n",
    "    196: \"Запрос-Хост ProbClick clickposition 7\",\n",
    "    197: \"Запрос-Хост ProbClick clickposition 8\",\n",
    "    198: \"Запрос-Хост ProbClick clickposition 9\",\n",
    "    199: \"Запрос-Хост ProbClick clickposition 10\",\n",
    "    200: \"Запрос-Хост Probclick showposition 1\",\n",
    "    201: \"Запрос-Хост Probclick showposition 2\",\n",
    "    202: \"Запрос-Хост Probclick showposition 3\",\n",
    "    203: \"Запрос-Хост Probclick showposition 4\",\n",
    "    204: \"Запрос-Хост Probclick showposition 5\",\n",
    "    205: \"Запрос-Хост Probclick showposition 6\",\n",
    "    206: \"Запрос-Хост Probclick showposition 7\",\n",
    "    207: \"Запрос-Хост Probclick showposition 8\",\n",
    "    208: \"Запрос-Хост Probclick showposition 9\",\n",
    "    209: \"Запрос-Хост Probclick showposition 10\",\n",
    "    210: \"Запрос-Документ CTR\",\n",
    "    211: \"Запрос-Документ AVGtime\",\n",
    "    212: \"Запрос-Документ FirstCTR\",\n",
    "    213: \"Запрос-Документ LastCTR\",\n",
    "    214: \"Запрос-Документ AVG Position\",\n",
    "    215: \"Запрос-Документ AVG Position Click\",\n",
    "    216: \"Запрос-Документ AVG showPosition if was Click\",\n",
    "    217: \"Запрос-Документ Количество показов за пределами топ 10/на все показы\",\n",
    "    218: \"Запрос-Документ Количество кликов за пределами топ 10/на все клики\",\n",
    "    219: \"Запрос-Документ LastProb\",\n",
    "    220: \"Запрос-Документ AVG click before show doc\",\n",
    "    221: \"Запрос-Документ AVG click after show doc\",\n",
    "    222: \"Запрос-Документ CTR@5\",\n",
    "    223: \"Запрос-Документ Количество не первых кликов/на все клики\",\n",
    "    224: \"Запрос-Документ PBM\",\n",
    "    225: \"Запрос-Документ CM\",\n",
    "    226: \"Запрос-Документ log(1+shows)\",\n",
    "    227: \"Запрос-Документ log(1+clicks)\",\n",
    "    228: \"Запрос-Документ UpProb\",\n",
    "    229: \"Запрос-Документ DowbProb\",\n",
    "    230: \"Запрос-Документ CTR position 1\",\n",
    "    231: \"Запрос-Документ CTR position 2\",\n",
    "    232: \"Запрос-Документ CTR position 3\",\n",
    "    233: \"Запрос-Документ CTR position 4\",\n",
    "    234: \"Запрос-Документ CTR position 5\",\n",
    "    235: \"Запрос-Документ CTR position 6\",\n",
    "    236: \"Запрос-Документ CTR position 7\",\n",
    "    237: \"Запрос-Документ CTR position 8\",\n",
    "    238: \"Запрос-Документ CTR position 9\",\n",
    "    239: \"Запрос-Документ CTR position 10\",\n",
    "    240: \"Запрос-Документ ProbClick clickposition 1\",\n",
    "    241: \"Запрос-Документ ProbClick clickposition 2\",\n",
    "    242: \"Запрос-Документ ProbClick clickposition 3\",\n",
    "    243: \"Запрос-Документ ProbClick clickposition 4\",\n",
    "    244: \"Запрос-Документ ProbClick clickposition 5\",\n",
    "    245: \"Запрос-Документ ProbClick clickposition 6\",\n",
    "    246: \"Запрос-Документ ProbClick clickposition 7\",\n",
    "    247: \"Запрос-Документ ProbClick clickposition 8\",\n",
    "    248: \"Запрос-Документ ProbClick clickposition 9\",\n",
    "    249: \"Запрос-Документ ProbClick clickposition 10\",\n",
    "    250: \"Запрос-Документ Probclick showposition 1\",\n",
    "    251: \"Запрос-Документ Probclick showposition 2\",\n",
    "    252: \"Запрос-Документ Probclick showposition 3\",\n",
    "    253: \"Запрос-Документ Probclick showposition 4\",\n",
    "    254: \"Запрос-Документ Probclick showposition 5\",\n",
    "    255: \"Запрос-Документ Probclick showposition 6\",\n",
    "    256: \"Запрос-Документ Probclick showposition 7\",\n",
    "    257: \"Запрос-Документ Probclick showposition 8\",\n",
    "    258: \"Запрос-Документ Probclick showposition 9\",\n",
    "    259: \"Запрос-Документ Probclick showposition 10\",\n",
    "    260: \"Запрос AVG DOCS\",\n",
    "    261: \"Запрос AVG Clicks\",\n",
    "    262: \"Запрос AVGTime\",\n",
    "    263: \"Запрос средняя позици клика\",\n",
    "    264: \"Запрос Средняя позиция первого клика\",\n",
    "    265: \"Запрос Сколько раз без клика\",\n",
    "    266: \"Похожесть по universal-sentence-encoder-multilingual-qa/3\",\n",
    "    267: \"Похожесть по universal-sentence-encoder-multilingual-large/3\",\n",
    "    268: \"Похожесть по universal-sentence-encoder-multilingual/3\",\n",
    "    269: \"translation похожесть по universal-sentence-encoder-qa/3\",\n",
    "    270: \"translation cos universal-sentence-encoder-large/5\",\n",
    "    271: \"Похожесть косинусная Bert distiluse-base-multilingual-cased\",\n",
    "    272: \"Похожесть косинусная Bert bert-large-nli-mean-tokens\",\n",
    "    273: \"Похожесть косинусная Bert roberta-large-nli-stsb-mean-tokens\",\n",
    "    274: \"bm25doc\",\n",
    "    275: \"transforms_cos_sum DeepPavlov/rubert-base-cased-conversational\",\n",
    "    276: \"transforms_sum_cos DeepPavlov/rubert-base-cased-conversational\",\n",
    "    277: \"transforms_cos_sum DeepPavlov/rubert-base-cased\",\n",
    "    278: \"transforms_sum_cos DeepPavlov/rubert-base-cased\",\n",
    "    279: \"transforms_cos_sum DeepPavlov/rubert-base-cased-sentence\",\n",
    "    280: \"transforms_sum_cos DeepPavlov/rubert-base-cased-sentence\",\n",
    "    281:\"bm25doc\",\n",
    "    282: \"Похожесть косинусная Bert distiluse-base-multilingual-cased, НЕНОРМИРОВАННЫЕ\",\n",
    "    283: \"Похожесть по universal-sentence-encoder-multilingual-qa/3,  НЕНОРМИРОВАННЫЕ\",\n",
    "    284: \"Похожесть по universal-sentence-encoder-multilingual-large/3,  НЕНОРМИРОВАННЫЕ\",\n",
    "    285: \"Похожесть по universal-sentence-encoder-multilingual/3,  НЕНОРМИРОВАННЫЕ\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(features):\n",
    "    qid_data=[]\n",
    "    y_data = []\n",
    "    doc_ids=[]\n",
    "    X_data=[]\n",
    "    for l in features.keys():\n",
    "        line=l.split(\"|\")\n",
    "        qid_data.append(int(line[0]))\n",
    "        y_data.append(float(line[2]))\n",
    "        doc_ids.append(int(line[1]))\n",
    "        X_data.append(features[l])\n",
    "    qid_data=np.array(qid_data)\n",
    "    y_data=np.array(y_data)\n",
    "    X_data= csr_matrix(X_data)\n",
    "    doc_ids= np.array(doc_ids)\n",
    "    group_data =np.unique(qid_data, return_counts=True)[1]\n",
    "    return X_data, y_data, qid_data, group_data, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train, qid_train, group_train, doc_ids_train = make_data(features_train)\n",
    "X_test,y_test, qid_test, group_test, doc_ids_test = make_data(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_features(file, features):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for i in features.keys():\n",
    "            iid = i.split('|')\n",
    "            f.write(iid[0]+'\\t'+iid[2]+'\\t'+iid[1]+'\\t')\n",
    "            for s in features[i]:\n",
    "                if not np.isfinite(s):\n",
    "                    s=0\n",
    "                f.write(str(s)+'\\t')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_features(\"Final_Test_data.txt\", features_test)\n",
    "save_all_features(\"Final_Train_data.txt\", features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    qid_data=[]\n",
    "    y_data = []\n",
    "    doc_ids=[]\n",
    "    X_data=[]\n",
    "    file = open(filename, 'r')\n",
    "    for l in file.readlines():\n",
    "        line=l.strip().split('\\t')\n",
    "        x=[]\n",
    "        #if float(line[3])==0.0:\n",
    "            #continue\n",
    "        for i in range(3, len(line)):\n",
    "#             if i-3 in skip:\n",
    "#                 continue\n",
    "            x.append(float(line[i].strip()))\n",
    "        if np.sum(x)==0.:\n",
    "            continue\n",
    "        qid_data.append(int(line[0]))\n",
    "        y_data.append(float(line[1]))\n",
    "        doc_ids.append(int(line[2]))\n",
    "        X_data.append(np.array(x))\n",
    "    qid_data=np.array(qid_data)\n",
    "    y_data=np.array(y_data)\n",
    "    X_data= csr_matrix(X_data)\n",
    "    doc_ids= np.array(doc_ids)\n",
    "    group_data =np.unique(qid_data, return_counts=True)[1]\n",
    "    file.close()\n",
    "    return X_data, y_data, qid_data, group_data, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train, qid_train, group_train, doc_ids_train = load_data(\"Final_Train_data.txt\")\n",
    "X_test,y_test, qid_test, group_test, doc_ids_test = load_data(\"Final_Test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':100,\n",
    "'subsample':0.9,\n",
    "'learning_rate':0.1,\n",
    "'num_leaves':63,}\n",
    "Ranker1 = gbm.LGBMRanker(**params)\n",
    "Ranker1.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':500,\n",
    "'subsample':0.9,\n",
    "'learning_rate':0.1,\n",
    "'num_leaves':127,}\n",
    "Ranker2 = gbm.LGBMRanker(**params)\n",
    "Ranker2.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':1000,\n",
    "'subsample':0.95,\n",
    "\n",
    "'learning_rate':0.01,\n",
    "'num_leaves':31,}\n",
    "Ranker3 = gbm.LGBMRanker(**params)\n",
    "Ranker3.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRanker(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "           importance_type='split', learning_rate=0.007, max_depth=-1,\n",
       "           min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "           n_estimators=3000, n_jobs=-1, num_leaves=29, objective=None,\n",
       "           random_state=None, reg_alpha=0.0, reg_lambda=0.0, score='ndcg@5',\n",
       "           silent=True, subsample=0.95, subsample_for_bin=200000,\n",
       "           subsample_freq=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params={'boosting_type':'gbdt','score':'ndcg@5',\n",
    "'n_estimators':3000,\n",
    "'subsample':0.95,\n",
    "'learning_rate':0.007,\n",
    "'num_leaves':29,}\n",
    "Ranker = gbm.LGBMRanker(**params)\n",
    "Ranker.fit(X_train, y_train, group=group_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =(R.predict(X_test)+R1.predict(X_test)+R2.predict(X_test)+R3.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(qid_test,pred,doc_ids_test, 'ans.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
