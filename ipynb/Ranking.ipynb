{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Ranking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJudv54cdk3-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pymystem3 import Mystem\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy.spatial.distance as ds\n",
        "import torch\n",
        "import fasttext.util\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import os\n",
        "from scipy.special import expit\n",
        "import lightgbm as gbm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n",
        "from stop_words import get_stop_words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import wget\n",
        "import gensim\n",
        "import zipfile\n",
        "import fasttext.util\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnz2u_X2dk4I"
      },
      "source": [
        "# извлекаем и лемматизируем тайтлы и запросы (запросы прогнал через спеллчекер яндекса, а также и тайтлы и запросы через Яндекс Переводчик, чтобы перевести на англ)\n",
        "\n",
        "def stem(doc):\n",
        "  txt = doc.split('\\t')[1].strip()\n",
        "  qid = doc.split('\\t')[0]\n",
        "  st = Mystem()\n",
        "  s = st.lemmatize(txt.lower())\n",
        "  return qid, ''.join(s)\n",
        "\n",
        "queries = {}\n",
        "non_norm_q = {}\n",
        "with open(\"Yandex_spell.txt\", 'r') as f:\n",
        "  txt = f.readlines()\n",
        "  for i, qid, text in enumerate(list(map(stem, txt))):\n",
        "    queries[int(qid)] = text.strip()\n",
        "    non_norm_q[int(qid)] = txt[i].split('\\t')[1].strip()\n",
        "\n",
        "titles = []\n",
        "dids = []\n",
        "non_norm_t = {}\n",
        "with open(\"titles.txt\", 'r') as f:\n",
        "  titles = f.readlines()\n",
        "  for i, did, txt in enumerate(list(map(stem, titles))):\n",
        "    titles.append(txt.strip())\n",
        "    dids.append(int(did))\n",
        "    non_norm_t[int(did)] = txt[i].split('\\t')[1].strip()\n",
        "  f.close()\n",
        "\n",
        "en_queries = {}\n",
        "en_titles = {}\n",
        "en_titles_arr = []\n",
        "en_dids = []\n",
        "with open(\"Yandex_translate_queries.txt\" ,'r') as f:\n",
        "  for line in f.readlines():\n",
        "    txt = line.split('\\t')\n",
        "    en_queries[int(txt[0])]= txt[1].lower().strip()\n",
        "  f.close()\n",
        "with open(\"Yandex_translate_titles.txt\" ,'r') as f:\n",
        "  for line in f.readlines():\n",
        "    txt = line.split('\\t')\n",
        "    en_titles_arr.append(txt[1].lower().strip())\n",
        "    en_dids.append(int(txt[0]))\n",
        "    en_titles[int(txt[0])]= txt[1].lower().strip()\n",
        "  f.close()\n",
        "\n",
        "train_data = []\n",
        "qmap = {}\n",
        "f = open(\"train.marks.tsv\", 'r')\n",
        "for line in f.readlines():\n",
        "    arr = line.split('\\t')\n",
        "    if int(arr[2]) == 0:\n",
        "        continue\n",
        "    train_data.append([int(arr[0]), int(arr[1]), int(arr[2])])\n",
        "    if not (int(arr[0]) in qmap.keys()):\n",
        "        qmap[int(arr[0])]=[int(arr[1])]\n",
        "    else:\n",
        "        qmap[int(arr[0])].append(int(arr[1]))\n",
        "train_data = np.array(train_data)\n",
        "f.close()\n",
        "\n",
        "f = open(\"sample.csv\", 'r')\n",
        "f.readline()\n",
        "test_data = []\n",
        "for line in f.readlines():\n",
        "    arr = line.split(',')\n",
        "    test_data.append([int(arr[0]), int(arr[1]), -1])\n",
        "    if int(arr[0]) not in qmap.keys():\n",
        "        qmap[int(arr[0])]=[int(arr[1])]\n",
        "    else:\n",
        "        qmap[int(arr[0])].append(int(arr[1]))\n",
        "test_data = np.array(test_data)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBgbvDddk4T"
      },
      "source": [
        "#каждому хосту ставим в соответствие hid\n",
        "hmap = {}\n",
        "with open(\"url.data\", 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    arr = line.split('\\t')\n",
        "    host = arr[1].split(\"/\")[0]\n",
        "    if host.startswith(\"www.\"):\n",
        "      host = host[4:] \n",
        "    did = int(arr[0])\n",
        "    hmap[host] = did\n",
        "\n",
        "#словарь из страниц для каждого хоста\n",
        "hosts = {}\n",
        "with open(\"url.data\", 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    arr = line.split('\\t')\n",
        "    host = arr[1].split(\"/\")[0]\n",
        "    if host.startswith(\"www.\"):\n",
        "      host = host[4:]\n",
        "    hid = hmap[host]    \n",
        "    did = int(arr[0])\n",
        "    if hid in hosts.keys():\n",
        "      hosts[hid].append(did)\n",
        "    else:\n",
        "      hosts[hid] = [did]\n",
        "\n",
        "#поведенческие фичи\n",
        "def get_beh_features(dir, query, host):\n",
        "    Pages = {}\n",
        "    for i in range(10):\n",
        "      with open(\"click_data/\"+dir+\"/part-r-000\"+str(i).zfill(2), 'r') as f:\n",
        "        for line in f.readlines():\n",
        "          data = {}\n",
        "          line = line.strip().split('\\t')\n",
        "          if query:\n",
        "              beg = 2\n",
        "              end = 11\n",
        "          else:\n",
        "              beg = 1\n",
        "              end = 10\n",
        "          for k in range(beg, end):\n",
        "              arr = line[k].strip().split(\":\")\n",
        "              data[arr[0]] = float(arr[1])\n",
        "          for k in range(end, end + 2):\n",
        "            arr = line[k].strip().split(\":\")\n",
        "            pos = []\n",
        "            for ps in enumerate(arr[1].strip().split(\" \")):\n",
        "                pos.append(int(ps))\n",
        "            data[arr[0]] = np.array(pos)\n",
        "          if query:\n",
        "                Pages[line[0]+','+line[1]] = data\n",
        "          else:\n",
        "                Pages[int(line[0])] = data\n",
        "    \n",
        "    features = {}\n",
        "    gamma = np.array([0.41, 0.16, 0.105, 0.08, 0.06])\n",
        "    for i in Pages.keys():\n",
        "        ctr = Pages[i]['clicks'] / Pages[i]['shows'] #отношение числа кликов к числу показов\n",
        "        first_ctr = Pages[i]['first_clicks'] / Pages[i]['shows'] #отношение первых кликов к числу показов\n",
        "        last_ctr = Pages[i]['last_clicks'] / Pages[i]['shows'] # отношение последних кликов к числу показов\n",
        "        mean_time = Pages[i]['time'] / Pages[i]['clicks'] #среднее время проведенное на странице\n",
        "        pos_ctr = Pages[i]['pos_clicks'] / Pages[i]['pos_shows'] #отношение числа кликов к числу показов для каждой из топ 5 позиций выдачи\n",
        "        not_top_shows = Docs[i]['shows_not_top'] / Docs[i]['shows'] #процент показов вне топ 5\n",
        "        not_top_clicks = Docs[i]['clicks_not_top'] / Docs[i]['clicks'] #процент кликов вне топ 5\n",
        "        not_top_ctr = not_top_clicks / not_top_shows #CTR вне топа\n",
        "        pos_click_probability = Pages[i]['pos_clicks'] / Pages[i]['clicks'] #проценты кликов от общего числа в топ 5\n",
        "        PBM = Docs[i]['clicks'] / np.sum(Docs[i]['shows_pos'] * gamma) #PBM\n",
        "        mean_after_clicks = 0\n",
        "        mean_before_clicks = 0\n",
        "        if Pages[i]['clicks'] - Pages[i]['first_clicks'] != 0:\n",
        "            mean_before_clicks = Pages[i]['before_clicks'] / (Pages[i]['clicks'] - Pages[i]['first_clicks']) #среднее кол-во кликов до перехода к тек. странице\n",
        "        if Pages[i]['clicks'] - Pages[i]['last_clicks'] != 0:\n",
        "            mean_after_clicks = Pages[i]['after_clicks'] / (Pages[i]['clicks'] - Pages[i]['last_clicks']) #среднее кол-во кликов после перехода к тек. странице\n",
        "        f = [ctr, first_ctr, last_ctr, mean_time, pos_ctr, not_top_shows, not_top_clicks, not_top_ctr, pos_click_probability, mean_before_clicks, mean_after_clicks, PBM]\n",
        "        if not host:\n",
        "            features[i] = f   \n",
        "        if host and query:\n",
        "            s = i.split(',')\n",
        "            if int(s[1]) in hosts.keys():\n",
        "                for doc in hosts[int(s[1])]:\n",
        "                    if doc in qmap[int(s[0])]:\n",
        "                        features[s[0]+\",\"+ str(doc)] = f\n",
        "        if host and not query:\n",
        "            if i in hosts.keys():\n",
        "                for doc in hosts[i]:\n",
        "                    features[doc] = f\n",
        "    return features\n",
        "\n",
        "docs_features = get_beh_features(\"docs\", False, False)\n",
        "hosts_features = get_beh_features(\"hosts\", False, True)\n",
        "qd_features= get_beh_features(\"query-docs\", True, False)\n",
        "qh_features = get_beh_features(\"query-hosts\", True, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2xVbqKdk4X"
      },
      "source": [
        "stop = get_stop_words('en') + get_stop_words('ru') + ['aren', 'can', 'isn', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven']\n",
        "\n",
        "#преобразуем текст в нграммы по словам или все сразу\n",
        "def parse_ngrams(text, n = 1, word = True):\n",
        "    if word:\n",
        "        n_grams = ngrams(word_tokenize(text), n)\n",
        "        return [' '.join(grams) for grams in n_grams]\n",
        "    else:\n",
        "        n_grams = ngrams(text, n)\n",
        "        return [''.join(grams) for grams in n_grams]\n",
        "\n",
        "#генератор возварщающий тайтлы документов по doc_ids на английском или русском\n",
        "def gen_titles(dids, bm25 = False, n = 1, word = True, english = False):\n",
        "    for i in dids:\n",
        "        if bm25:\n",
        "          if english:\n",
        "            yield parse_ngrams(en_titles[i], n=n, word=word)\n",
        "          else:\n",
        "            yield parse_ngrams(titles[i], n=n, word=word)\n",
        "        else:\n",
        "            yield titles[i]\n",
        "\n",
        "#разделил доки по запросам, это генератор, возвращающий тексты доков \n",
        "def gen_docs_qid(qid):\n",
        "    docs = {}\n",
        "    f = open(\"documents_by_qid/{}.txt\".format(qid), 'r')\n",
        "    for line in f.readlines():\n",
        "        arr = line.split('\\t')\n",
        "        docs[int(arr[0])] = arr[1].strip()\n",
        "    f.close()\n",
        "    for i in qmap[qid]:\n",
        "        if i in doc.keys():\n",
        "            yield docs[i]\n",
        "    del docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2ICJoQyOdk4Y"
      },
      "source": [
        "#фичи BM25 и TF * IDF на тайтлах\n",
        "tfidfbm25_features = {}\n",
        "for i in range(len(qmap)):\n",
        "    vectorizer = TfidfVectorizer(use_idf = False)\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f1 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f2 = cosine_similarity(D, q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop, use_idf = False)\n",
        "    D=vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q=vectorizer.transform([queries[i]])\n",
        "\n",
        "    f3 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f4 = cosine_similarity(D,q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop, use_idf = False)\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f5 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f6 = cosine_similarity(D,q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(3,9), stop_words=stop, analyzer='char_wb', use_idf = False)\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f7 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f8 = cosine_similarity(D,q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f9 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f10 = cosine_similarity(D, q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words=stop)\n",
        "    D=vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q=vectorizer.transform([queries[i]])\n",
        "\n",
        "    f11 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f12 = cosine_similarity(D,q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=stop)\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f13 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f14 = cosine_similarity(D,q).flatten()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(3,9), stop_words=stop, analyzer='char_wb')\n",
        "    D = vectorizer.fit_transform(gen_titles(qmap[i]))\n",
        "    q = vectorizer.transform([queries[i]])\n",
        "\n",
        "    f15 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "    f16 = cosine_similarity(D,q).flatten()\n",
        "    \n",
        "    \n",
        "      \n",
        "    \n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 1))\n",
        "    bm25 = BM25Plus(grammas)\n",
        "    f17 = bm25.get_scores(parse_ngrams(queries[i], n = 1, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2))\n",
        "    bm25 = BM25Plus(grammas)\n",
        "    f18 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2, word = False))\n",
        "    bm25 = BM25Plus(grammas)\n",
        "    f19 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 3, word = False))\n",
        "    bm25 = BM25Plus(grammas)\n",
        "    f20 = bm25.get_scores(parse_ngrams(queries[i], n = 3, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 5, word = False))\n",
        "    bm25 = BM25Plus(grammas)\n",
        "    f21 = bm25.get_scores(parse_ngrams(queries[i], n = 5, word = False))\n",
        "    \n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 1))\n",
        "    bm25 = BM25L(grammas)\n",
        "    f22 = bm25.get_scores(parse_ngrams(queries[i], n = 1, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2))\n",
        "    bm25 = BM25L(grammas)\n",
        "    f23 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2, word = False))\n",
        "    bm25 = BM25L(grammas)\n",
        "    f24 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 3, word = False))\n",
        "    bm25 = BM25L(grammas)\n",
        "    f25 = bm25.get_scores(parse_ngrams(queries[i], n = 3, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 5, word = False))\n",
        "    bm25 = BM25L(grammas)\n",
        "    f26 = bm25.get_scores(parse_ngrams(queries[i], n = 5, word = False))\n",
        "\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 1))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f27 = bm25.get_scores(parse_ngrams(queries[i], n = 1, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f28 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = True))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 2, word = False))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f29 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 3, word = False))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f30 = bm25.get_scores(parse_ngrams(queries[i], n = 3, word = False))\n",
        "\n",
        "    grammas = list(gen_titles(qmap[i], bm25 = True, n = 5, word = False))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f31 = bm25.get_scores(parse_ngrams(queries[i], n = 5, word = False))\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    for k, j in enumerate(qmap[i]):\n",
        "        tfidfbm25_features[str(i) + \",\" + str(j)] = [f1[k], f2[k], f3[k], f4[k], f5[k], f6[k], f7[k], f8[k], f9[k], f10[k], f11[k], f12[k], f13[k], f14[k], f15[k], f16[k], f17[k], f18[k], f19[k], f20[k], f21[k], f22[k], f23[k], f24[k], f25[k], f26[k], f27[k], f28[k], f29[k], f30[k], f31[k]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TdZIrpldk4Y"
      },
      "source": [
        "#фичи BM25 и TF * IDF на текстах доков, особо много фичей не сделаешь - долго считает\n",
        "def tfidfbm25_doc(qid):\n",
        "    vectorizer = TfidfVectorizer(ngram_range = (1, 3), stop_words = stop)\n",
        "    D = vectorizer.fit_transform(gen_docs_qid(qid))\n",
        "    q = vectorizer.transform([queries[qid]])\n",
        "    f1 = cosine_similarity(D, q).flatten()\n",
        "    f2 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range = (1, 1), stop_words = stop)\n",
        "    D = vectorizer.fit_transform(gen_docs_qid(qid))\n",
        "    q = vectorizer.transform([queries[qid]])\n",
        "    f3 = cosine_similarity(D, q).flatten()\n",
        "    f4 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range = (2, 2), stop_words = stop)\n",
        "    D = vectorizer.fit_transform(gen_docs_qid(qid))\n",
        "    q = vectorizer.transform([queries[qid]])\n",
        "    f5 = cosine_similarity(D, q).flatten()\n",
        "    f6 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(ngram_range = (3, 9), stop_words = stop, analyzer = 'char_wb')\n",
        "    D = vectorizer.fit_transform(gen_docs_qid(qid))\n",
        "    q = vectorizer.transform([queries[qid]])\n",
        "    f7 = cosine_similarity(D,q).flatten()\n",
        "    f8 = (((D.dot(q.T)).toarray()).T).reshape(D.shape[0])\n",
        "\n",
        "    grammas = list(gen_docs_qid(qid))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f9 = bm25.get_scores(parse_ngrams(queries[i], n = 1, word = True))\n",
        "\n",
        "    grammas = list(gen_docs_qid(qid))\n",
        "    bm25 = BM25Okapi(grammas)\n",
        "    f10 = bm25.get_scores(parse_ngrams(queries[i], n = 2, word = True))\n",
        "\n",
        "    return qid, np.vstack((f1, f2, f3, f4, f5, f6, f7, f8, f9, f10))\n",
        "\n",
        "\n",
        "doc_tdidfbm25_features={}\n",
        "for i, f in list(map(tfidfbm25_doc, range(6311))):\n",
        "    for k, j in zip(qmap[i], f):\n",
        "        doc_tdidfbm25_features[str(i) + \",\" + str(k)] = j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqz8IDhidk4c"
      },
      "source": [
        "#модели фасттекст\n",
        "en_model = fasttext.load_model('cc.en.300.bin')\n",
        "ru_model = fasttext.load_model('cc.ru.300.bin')\n",
        "ru_model_news = fasttext.load_model('ft_native_300_ru_wiki_lenta_lemmatize.bin') #модель из системы рекомендации новостей https://github.com/temur-kh/news-recommendation-system\n",
        "\n",
        "\n",
        "#фичи фасттекст\n",
        "def features_ft(qid):\n",
        "    features = []\n",
        "    v1_q = np.zeros(300)\n",
        "    v2_q = np.zeros(300)\n",
        "    v3_q = np.zeros(300)\n",
        "    v4_q = ru_model.get_sentence_vector(queries[qid])\n",
        "    v5_q = en_model.get_sentence_vector(queries[qid])\n",
        "    v6_q = ru_model_news.get_sentence_vector(queries[qid])\n",
        "    for word in queries[qid].split():\n",
        "        v1_q += ru_model.get_word_vector(word)\n",
        "        v2_q += en_model.get_word_vector(word)\n",
        "        v3_q += ru_model_news.get_word_vector(word)\n",
        "    for title in list(gen_titles(qmap[qid])):\n",
        "        v1_d = np.zeros(300)\n",
        "        v2_d = np.zeros(300)\n",
        "        v3_d = np.zeros(300)\n",
        "        v4_d = ru_model.get_sentence_vector(title)\n",
        "        v5_d = en_model.get_sentence_vector(title)\n",
        "        v6_d = ru_model_news.get_sentence_vector(title)\n",
        "        for word in title.split():\n",
        "            v1_d += ru_model.get_word_vector(word)\n",
        "            v2_d += en_model.get_word_vector(word)\n",
        "            v3_d += ru_model_news.get_word_vector(word)\n",
        "        feaatures.append([np.dot(v1_q,v1_d), np.dot(v2_q,v2_d), np.dot(v3_q,v3_d), np.dot(v4_q,v4_d), np.dot(v5_q,v5_d), np.dot(v6_q,v6_d)\n",
        "                          ds.cosine(v1_q,v1_d), ds.cosine(v2_q,v2_d), ds.cosine(v3_q,v3_d), ds.cosine(v4_q,v4_d), ds.cosine(v5_q,v5_d), ds.cosine(v6_q,v6_d)]\n",
        "    return features\n",
        "\n",
        "features_fasttext = {}\n",
        "for qid in range(6311):\n",
        "    for i, features in zip(qmap[qid], features_ft(qid)):\n",
        "        fasttext_features[str(qid) + \",\" + str(i)] = features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgATJmfydk4d"
      },
      "source": [
        "#bert\n",
        "transform_model = SentenceTransformer('bert-large-nli-mean-tokens', device='cuda')\n",
        "vecs = transform_model.encode(en_titles_arr, batch_size=128)\n",
        "tr_titles = {}\n",
        "for did, vec in zip(en_dids, vecs):\n",
        "    tr_titles[did] = vec\n",
        "\n",
        "bert1_ft = {}\n",
        "for qid in range(6311):\n",
        "    qvecs = transform_model.encode([queries[qid]])[0]\n",
        "    for i in qmap[qid]:\n",
        "        if i in tr_titles.keys():\n",
        "            bert1_ft[str(qid)+\",\"+str(i)] = ds.cosine(qvecs, tr_titles[i])\n",
        "\n",
        "#roberta\n",
        "transform_model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens', device='cuda')\n",
        "vecs = transform_model.encode(en_titles_arr, batch_size=128)\n",
        "tr_titles = {}\n",
        "for did, vec in zip(en_dids, vecs):\n",
        "    tr_titles[did] = vec\n",
        "\n",
        "bert2_ft = {}\n",
        "for qid in range(6311):\n",
        "    qvecs = transform_model.encode([queries[qid]])[0]\n",
        "    for i in qmap[qid]:\n",
        "        if i in tr_titles.keys():\n",
        "            bert2_ft[str(qid)+\",\"+str(i)] = ds.cosine(qvecs, tr_titles[i])\n",
        "\n",
        "#многоязычный USE\n",
        "transform_model = SentenceTransformer('distiluse-base-multilingual-cased', device = 'cuda')\n",
        "vecs = transform_model.encode(titles, batch_size=128)\n",
        "tr_titles = {}\n",
        "for did, vec in zip(dids, vecs):\n",
        "    tr_titles[did] = vec\n",
        "\n",
        "bert3_ft = {}\n",
        "for qid in range(6311):\n",
        "    qvecs = transform_model.encode([queries[qid]])[0]\n",
        "    for i in qmap[qid]:\n",
        "        if i in tr_titles.keys():\n",
        "            bert3_ft[str(qid)+\",\"+str(i)] = ds.cosine(qvecs, tr_titles[i]))\n",
        "\n",
        "\n",
        "#склеил фичи\n",
        "bert_features = {}\n",
        "for i in bert1_ft.keys():\n",
        "    bert_features[i] = [bert1_ft[i]]\n",
        "    if i in bert2_ft.keys():\n",
        "        bert_features[i].append(bert2_ft[i])\n",
        "    else:\n",
        "        bert_features[i].append(1)\n",
        "    if i in bert3_ft.keys():\n",
        "        bert_features[i].append(bert3_ft[i])\n",
        "    else:\n",
        "        bert_features[i].append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAmxWN3bdk4g"
      },
      "source": [
        "#всякие USE но теперь из tensorflow\n",
        "qa_m_trans = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
        "qa_trans= hub.load('https://tfhub.dev/google/universal-sentence-encoder-qa/3')\n",
        "trans = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
        "m_trans = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "lm_trans = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
        "\n",
        "\n",
        "def features_use(qid):\n",
        "    q = [queries[qid]]\n",
        "    qvecs = qa_m_trans.signatures['question_encoder'](tf.constant(q))\n",
        "    titles = list(gen_titles(qmap[qid]))\n",
        "    D = list(gen_docs_qid(qid))\n",
        "    tvecs = qa_m_trans.signatures['response_encoder'](input = tf.constant(titles), context = tf.constant(D))\n",
        "    f1 = np.inner(qvecs['outputs'], tvecs['outputs']).flatten().reshape(-1, 1)\n",
        "\n",
        "    q = [en_queries[qid]]\n",
        "    titles = list(gen_titles(qmap[qid], english = True))\n",
        "    D = list(gen_titles(qmap[qid], english = True))\n",
        "    qvecs = qa_trans.signatures['question_encoder'](tf.constant(q))\n",
        "    tvecs = qa_trans.signatures['response_encoder'](input = tf.constant(titles), context = tf.constant(D))\n",
        "    f2 = np.inner(qvecs['outputs'], tvecs['outputs']).flatten().reshape(-1, 1)\n",
        "\n",
        "    f3 = cosine_similarity(lm_trans(queries[qid]), lm_trans(list(gen_titles(qmap[qid])))).flatten().reshape(-1, 1)\n",
        "    f4 = cosine_similarity(m_trans(queries[qid]), m_trans(list(gen_titles(qmap[qid])))).flatten().reshape(-1, 1)\n",
        "    f5 = cosine_similarity(trans([en_queries[qid]])[0].reshape(1,-1), trans(list(gen_titles(qmap[qid], english = True)))).flatten().reshape(-1, 1)\n",
        "    return np.hstack((f1, f2, f3, f4, f5))\n",
        "\n",
        "use_features={}\n",
        "for qid in tqdm.tqdm(range(6311)):\n",
        "    for did, vec in zip(qmap[qid], features_use(qid)):\n",
        "        use_features[str(qid)+\",\"+str(did)] = vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFXvdwcgdk4j"
      },
      "source": [
        "#соединяем готовые фичи\n",
        "def get_features(data):\n",
        "    features = {}\n",
        "    for i in data:\n",
        "        f1 = np.zeros(31)\n",
        "        if str(i[0])+\",\"+str(i[1]) in tfidfbm25_features.keys():\n",
        "            f1 = tfidfbm25_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f2 = np.zeros(10)\n",
        "        if str(i[0])+\",\"+str(i[1]) in doc_tdidfbm25_features.keys():\n",
        "            f2 = doc_tdidfbm25_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f3 = np.zeros(12)\n",
        "        if str(i[0])+\",\"+str(i[1]) in fasttext_features.keys():\n",
        "            f3 = fasttext_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f4 = np.zeros(3)\n",
        "        if str(i[0])+\",\"+str(i[1]) in bert_features.keys():\n",
        "            f4 = bert_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f5 = np.zeros(5)\n",
        "        if str(i[0])+\",\"+str(i[1]) in use_features.keys():\n",
        "            f5 = use_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f6 = np.zeros(12)\n",
        "        if str(i[0])+\",\"+str(i[1]) in docs_features.keys():\n",
        "            f6 = docs_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f7 = np.zeros(12)\n",
        "        if str(i[0])+\",\"+str(i[1]) in hosts_features.keys():\n",
        "            f7 = hosts_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f8 = np.zeros(12)\n",
        "        if str(i[0])+\",\"+str(i[1]) in qd_features.keys():\n",
        "            f8 = qd_features[str(i[0])+\",\"+str(i[1])]\n",
        "        f9 = np.zeros(12)\n",
        "        if str(i[0])+\",\"+str(i[1]) in qh_features.keys():\n",
        "            f9 = qh_features[str(i[0])+\",\"+str(i[1])]\n",
        "        features[str(i[0])+\",\"+str(i[1])+\",\"+str(i[2])] = np.hstack((f1, f2, f3, f4, f5, f6, f7, f8, f9))\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jEhSNoxdk4k"
      },
      "source": [
        "def parse_features(features):\n",
        "  X =[]\n",
        "  y = []\n",
        "  qids =[]\n",
        "  doc_ids =[]\n",
        "  for key in features.keys():\n",
        "    X.append(features[key])\n",
        "    k = key.split(\",\")\n",
        "    y.append(float(k[2]))\n",
        "    qids.append(int(k[0]))\n",
        "    doc_ids.append(int(k[1]))\n",
        "  qids = np.array(qids)\n",
        "  y = np.array(y)\n",
        "  X = np.array(X)\n",
        "  doc_ids= np.array(doc_ids)\n",
        "  group_counts = np.unique(qids, return_counts=True)[1]\n",
        "  return X, y, qids, group_counts, doc_ids\n",
        "\n",
        "X_train, y_train, _, group_counts, _ = parse_features(get_features(train_data))\n",
        "X_test, _, qids, _, doc_ids = parse_features(get_features(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "itl3wPnudk4l"
      },
      "source": [
        "Rank1 = gbm.LGBMRanker(score='ndcg@5', n_estimators=100, num_leaves=127, learning_rate=0.1)\n",
        "Rank1.fit(X_train, y_train, group=group_counts)\n",
        "Rank2 = gbm.LGBMRanker(score='ndcg@5', n_estimators=500, num_leaves=127, learning_rate=0.1)\n",
        "Rank2.fit(X_train, y_train, group=group_counts)\n",
        "Rank3 = gbm.LGBMRanker(score='ndcg@5', n_estimators=1000, num_leaves=63, learning_rate=0.01)\n",
        "Rank3.fit(X_train, y_train, group=group_counts)\n",
        "Rank4 = gbm.LGBMRanker(score='ndcg@5', n_estimators=2000, num_leaves=31, learning_rate=0.01)\n",
        "Rank4.fit(X_train, y_train, group=group_counts)\n",
        "Rank5 = gbm.LGBMRanker(score='ndcg@5', n_estimators=4000, num_leaves=31, learning_rate=0.003)\n",
        "Rank5.fit(X_train, y_train, group=group_counts)\n",
        "y_pred = Rank1.predict(X_test) + Rank2.predict(X_test) + Rank3.predict(X_test) + Rank4.predict(X_test) + Rank5.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFlt9c0Cdk4m"
      },
      "source": [
        "out = open('out.txt', 'w')\n",
        "out.write('QueryId,DocumentId\\n')\n",
        "for qid in np.unique(qids):\n",
        "  docs_id = copy.deepcopy(doc_ids[np.argwhere(qids == qid).flatten()])\n",
        "  relevancy = y_pred[np.argwhere(qids == qid).flatten()]\n",
        "  docs_id = docs_id[np.argsort(relevancy)[::-1]]\n",
        "  i = 0\n",
        "  for doc_id in docs_id:\n",
        "    if i == 5:\n",
        "      break\n",
        "    out.write('{},{}\\n'.format(qid, doc_id))\n",
        "    i += 1\n",
        "out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}